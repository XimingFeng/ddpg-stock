{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_env import StockEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from replay_buffer import Buffer\n",
    "from ddpg import ActorCritic\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = {\n",
    "    \"asset_codes\": ['AAPL', 'V', 'BABA', 'ADBE', 'SNE'],\n",
    "    \"features\": [\"close\", \"high\", \"low\"],\n",
    "    \"start_date\": \"2017-5-1\",\n",
    "    \"end_date\": \"2017-6-23\", \n",
    "    \"window_len\": 50,\n",
    "    \"data_path\": \"AmericaStock.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_env = StockEnv(**env_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of states: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'total number of states: {len(stock_env.states)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dimention (5, 50, 3)\n",
      "action dimention (6,)\n"
     ]
    }
   ],
   "source": [
    "state_dim = tuple(stock_env.states[0].shape[1: ])\n",
    "action_dim = (len(env_args['asset_codes']) + 1, )\n",
    "print(f'state dimention {state_dim}')\n",
    "print(f'action dimention {action_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Network Summary: \n",
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_49 (InputLayer)           [(None, 5, 50, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 5, 50, 32)    128         input_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 5, 50, 32)    128         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 5, 50, 32)    1056        batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 5, 50, 32)    128         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_64 (ReLU)                 (None, 5, 50, 32)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 5, 50, 32)    1056        re_lu_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 5, 50, 32)    0           conv2d_98[0][0]                  \n",
      "                                                                 batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_65 (ReLU)                 (None, 5, 50, 32)    0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 8000)         0           re_lu_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 256)          2048256     flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256)          0           dense_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 6)            1542        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_16 (Softmax)            (None, 6)            0           dense_81[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,052,294\n",
      "Trainable params: 2,052,166\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n",
      "Critic Network Summary: \n",
      "Model: \"model_33\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_50 (InputLayer)           [(None, 5, 50, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 5, 50, 32)    128         input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 5, 50, 32)    128         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 5, 50, 32)    1056        batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 5, 50, 32)    128         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_66 (ReLU)                 (None, 5, 50, 32)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 5, 50, 32)    1056        re_lu_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 5, 50, 32)    0           conv2d_101[0][0]                 \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_67 (ReLU)                 (None, 5, 50, 32)    0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 8000)         0           re_lu_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_82 (Dense)                (None, 256)          2048256     flatten_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           dense_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_83 (Dense)                (None, 6)            1542        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_51 (InputLayer)           [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_34 (TensorFlowO [(None, 6)]          0           dense_83[0][0]                   \n",
      "                                                                 input_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_84 (Dense)                (None, 512)          3584        tf_op_layer_add_34[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 1)            513         dense_84[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,056,391\n",
      "Trainable params: 2,056,263\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_critic = ActorCritic(state_dim, action_dim, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_env.reset()\n",
    "state0 = stock_env.states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.0293806  0.08401833 0.12408818 0.48286393 0.11546466 0.16418421]], shape=(1, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "action0 = actor_critic.main_actor(state0)\n",
    "print(action0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action0 = np.squeeze(action0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, done, state1 = stock_env.step(action0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_args = {\n",
    "    'state_dim': state_dim,\n",
    "    'action_dim': action_dim\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_buffer = Buffer(**buffer_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_buffer.insert(state0, action0, reward, state1, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = stock_buffer.sample_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.one_step_train(train_batch, actor_optimizer, critic_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    \"env\": stock_env,\n",
    "    \"num_eps\": 20,\n",
    "    \"actor_lr\": 0.00001,\n",
    "    \"critic_lr\": 0.0001,\n",
    "    \"train_every_step\": 1,\n",
    "    \"batch_size\": 1,\n",
    "    \"verbose\": True, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, take action [0.17249607 0.3047047  0.06427959 0.09446795 0.19902344 0.16502827], receive reward -0.007135758145050558\n",
      "critic loss 8.90993105713278e-05\n",
      "step 1, take action [1.7502789e-05 9.9994421e-01 4.5521746e-12 1.8818406e-13 3.8226473e-05\n",
      " 4.4497203e-15], receive reward 0.0034253098730827776\n",
      "critic loss 273533536.0\n",
      "step 2, take action [1.4952620e-08 1.0000000e+00 2.3712484e-19 8.7527593e-22 5.1884477e-08\n",
      " 1.0560687e-24], receive reward -0.0041538824461249515\n",
      "critic loss 324928864.0\n",
      "step 3, take action [5.7302770e-11 1.0000000e+00 4.4215693e-25 2.4579679e-28 2.8900235e-10\n",
      " 2.8536918e-32], receive reward 0.0019613902260496247\n",
      "critic loss 239450352.0\n",
      "Episode 0, reward: -0.005902940492043107\n",
      "step 0, take action [5.6347001e-13 1.0000000e+00 7.2920583e-30 8.4971454e-34 3.8231020e-12\n",
      " 1.4278344e-38], receive reward -0.011656065788529818\n",
      "critic loss 12588799.0\n",
      "step 1, take action [1.2380566e-15 1.0000000e+00 1.2558009e-34 0.0000000e+00 8.1920537e-15\n",
      " 9.8090893e-45], receive reward 0.0034247908581988788\n",
      "critic loss 29870218.0\n",
      "step 2, take action [4.366223e-18 1.000000e+00 0.000000e+00 0.000000e+00 2.909787e-17\n",
      " 0.000000e+00], receive reward -0.0041539422988771495\n",
      "critic loss 68197336.0\n",
      "step 3, take action [3.7108078e-21 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2586299e-20\n",
      " 0.0000000e+00], receive reward 0.0019613902260496247\n",
      "critic loss 32.44020080566406\n",
      "Episode 1, reward: -0.010423827003158463\n",
      "step 0, take action [5.7326660e-24 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0127103e-23\n",
      " 0.0000000e+00], receive reward -0.011656065788529818\n",
      "critic loss 1157937.875\n",
      "step 1, take action [1.1279921e-27 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.5992569e-28\n",
      " 0.0000000e+00], receive reward 0.0034247908581988788\n",
      "critic loss 0.6847003698348999\n",
      "step 2, take action [1.1768455e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4312492e-32\n",
      " 0.0000000e+00], receive reward -0.0041539422988771495\n",
      "critic loss 101998.3828125\n",
      "step 3, take action [1.5581838e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1631494e-36\n",
      " 0.0000000e+00], receive reward 0.0019613902260496247\n",
      "critic loss 203423.015625\n",
      "Episode 2, reward: -0.010423827003158463\n",
      "step 0, take action [0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 2.51306e-40 0.00000e+00], receive reward -0.011656065788529818\n",
      "critic loss 311789.3125\n",
      "step 1, take action [0.0e+00 1.0e+00 0.0e+00 0.0e+00 6.6e-44 0.0e+00], receive reward 0.0034247908581988788\n",
      "critic loss 378868.34375\n",
      "step 2, take action [0. 1. 0. 0. 0. 0.], receive reward -0.0041539422988771495\n",
      "critic loss 328617.8125\n",
      "step 3, take action [0. 1. 0. 0. 0. 0.], receive reward 0.0019613902260496247\n",
      "critic loss 188275.953125\n",
      "Episode 3, reward: -0.010423827003158463\n",
      "step 0, take action [0. 1. 0. 0. 0. 0.], receive reward -0.011656065788529818\n",
      "critic loss 29214.439453125\n",
      "step 1, take action [0. 1. 0. 0. 0. 0.], receive reward 0.0034247908581988788\n",
      "critic loss 67.24755096435547\n",
      "step 2, take action [0. 1. 0. 0. 0. 0.], receive reward -0.0041539422988771495\n",
      "critic loss 222.7119140625\n",
      "step 3, take action [0. 1. 0. 0. 0. 0.], receive reward 0.0019613902260496247\n",
      "critic loss 1590.710205078125\n",
      "Episode 4, reward: -0.010423827003158463\n",
      "step 0, take action [0. 1. 0. 0. 0. 0.], receive reward -0.011656065788529818\n",
      "critic loss 4325.8369140625\n",
      "step 1, take action [0. 1. 0. 0. 0. 0.], receive reward 0.0034247908581988788\n",
      "critic loss 8485.677734375\n",
      "step 2, take action [0. 1. 0. 0. 0. 0.], receive reward -0.0041539422988771495\n",
      "critic loss 13944.447265625\n",
      "step 3, take action [0. 1. 0. 0. 0. 0.], receive reward 0.0019613902260496247\n",
      "critic loss nan\n",
      "Episode 5, reward: -0.010423827003158463\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 6, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 7, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 8, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 9, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 10, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 11, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 12, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 13, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 14, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 15, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 16, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 17, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 18, reward: nan\n",
      "step 0, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 2, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "step 3, take action [nan nan nan nan nan nan], receive reward nan\n",
      "critic loss nan\n",
      "Episode 19, reward: nan\n"
     ]
    }
   ],
   "source": [
    "rewards = actor_critic.train(**train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpg",
   "language": "python",
   "name": "ddpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
